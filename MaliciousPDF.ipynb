{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cdbbc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import textract\n",
    "import pdfplumber\n",
    "import base64\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import pdfid\n",
    "from collections import Counter\n",
    "from os import path\n",
    "from glob import glob  \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "from sklearn.metrics import roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c67a4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Basic Code Snippet that is used to extract structure of the pdf\\n#From this we identify keywords with / key before them using the regex.\\n#Uncomment and run this to see the raw pdf data\\n\\nimport base64\\nimport re\\nimport io\\n\\nfilename = \\'MaliciousPDF.pdf\\'\\nfilenameV2 = \\'MaliciousPDF.txt\\'\\nencoding = \\'utf-8\\'\\nregex = \\'\\\\/[^\\\\s\\n\\r]+\\\\s\\'\\n\\nwith open(filename, mode=\"r\",encoding=\\'utf-8\\',errors=\\'ignore\\') as pdf_file:\\n    encoded_string = pdf_file.readlines()\\n    print(encoded_string)\\n    \\n    with io.open(filenameV2, \"w\", encoding=\"utf-8\") as f:\\n        f.writelines(encoded_string)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Basic Code Snippet that is used to extract structure of the pdf\n",
    "#From this we identify keywords with / key before them using the regex.\n",
    "#Uncomment and run this to see the raw pdf data\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import io\n",
    "\n",
    "filename = 'MaliciousPDF.pdf'\n",
    "filenameV2 = 'MaliciousPDF.txt'\n",
    "encoding = 'utf-8'\n",
    "regex = '\\/[^\\s\\n\\r]+\\s'\n",
    "\n",
    "with open(filename, mode=\"r\",encoding='utf-8',errors='ignore') as pdf_file:\n",
    "    encoded_string = pdf_file.readlines()\n",
    "    print(encoded_string)\n",
    "    \n",
    "    with io.open(filenameV2, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(encoded_string)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83fc962c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tkarthik\\Project\\file.pdf\n",
      "C:\\Users\\tkarthik\\Project\\file1.pdf\n",
      "C:\\Users\\tkarthik\\Project\\FormsAPIReference.pdf\n",
      "C:\\Users\\tkarthik\\Project\\MaliciousPDF.pdf\n",
      "(4, 858)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tkarthik\\project\\maliciouspdf\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 322.  322.  178. ...    2.    2.    2.]\n",
      " [1028. 1028.    0. ...    0.    0.    0.]\n",
      " [  30.   30.    8. ...    0.    0.    0.]]\n",
      "[1 1 0]\n",
      "[[30. 30.  8.  6.  6.  4.  4.  4.  4.  4.  4.  4.  2.  2.  2.  2.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0. 15. 15.  4.  3.  3.  2.  2.  2.  2.  2.  2.  1.  1.  1.  1.\n",
      "   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "   1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.]]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Here we use regex to find all possible keywords and \n",
    "# Then return the ones without any special character or digit\n",
    "# This is feature extraction part.\n",
    "\n",
    "regex = '(endobj|obj|endstream|stream|startxref|xref|trailer|\\/[^\\s\\n\\r]+)'\n",
    "\n",
    "def FilteringFunction(match):\n",
    "    return match[1:].isalnum()\n",
    "\n",
    "def FilteringFunctionV2(match):\n",
    "    return match[0][1:].isalnum()\n",
    "\n",
    "def FilteringFunctionOnlyAlphabets(match):\n",
    "    return match[1:].isalpha()\n",
    "\n",
    "def Obfuscate(text) : \n",
    "    \n",
    "    newlist = []\n",
    "\n",
    "    if \"#\" in text:\n",
    "        x = text.split(\"#\")\n",
    "\n",
    "        for item in x:\n",
    "            if item == \"\":\n",
    "                continue\n",
    "\n",
    "            if not (item[0] >= '0' and item[0] <= '9'):\n",
    "                newlist.append(item)\n",
    "                continue\n",
    "\n",
    "            strlen = len(item)\n",
    "            count = int(0)\n",
    "            \n",
    "            while count < strlen and (item[count] >='0' and item[count] <= '9'):\n",
    "                count = count + 1\n",
    "            \n",
    "            ascii_string = chr(int(item[0:count], 16))\n",
    "            ascii_string = ascii_string + item[count:strlen]\n",
    "            newlist.append(ascii_string)\n",
    "    else :\n",
    "        newlist.append(text)\n",
    "    \n",
    "    return ''.join(newlist)\n",
    "\n",
    "\n",
    "\n",
    "def GetKeywords(fileNamePDF) :\n",
    "    \n",
    "    with open(fileNamePDF, mode=\"r\",encoding='utf-8',errors='ignore') as pdf_file:\n",
    "        print(fileNamePDF)\n",
    "        encoded_string = pdf_file.read()\n",
    "        matches = re.findall(regex, encoded_string)\n",
    "        obfuscatedMatches = list(map(Obfuscate, matches))\n",
    "        mergeResults = matches + obfuscatedMatches\n",
    "        #print(len(list(filter(FilteringFunctionV2, list(Counter(obfuscatedMatches).items())))))\n",
    "        \n",
    "        filteredMatches = list(filter(FilteringFunction, mergeResults))\n",
    "        return filteredMatches, len(encoded_string) , obfuscatedMatches\n",
    "\n",
    "def weightage(number_of_times_word_appeared, textLength, number_of_documents=1):\n",
    "    \n",
    "    tf = number_of_times_word_appeared/float(textLength)\n",
    "    idf = np.log((number_of_documents)/float(number_of_times_word_appeared))\n",
    "    tf_idf = tf*idf\n",
    "    return tf,idf ,tf_idf\n",
    "\n",
    "def GetKeywordVector(matches, textLength):\n",
    "    \n",
    "    matchesCount = Counter(matches)\n",
    "    data_items = matchesCount.items()\n",
    "    data_list = list(data_items)\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    df['tf'] = df[1].apply(lambda x: weightage(x,textLength)[0])\n",
    "    df['idf'] = df[1].apply(lambda x: weightage(x,textLength)[1])\n",
    "    df['tf_idf'] = df[1].apply(lambda x: weightage(x,textLength)[2])\n",
    "    df = df.sort_values('tf_idf',ascending=True)\n",
    "    #print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "KeyWordDictionary = {}\n",
    "KeyWordIndex = 0\n",
    "KeyWordList = []\n",
    "\n",
    "def resetKeyWordContainers():\n",
    "    global KeyWordDictionary\n",
    "    global KeyWordIndex\n",
    "    \n",
    "    KeyWordDictionary.clear()\n",
    "    KeyWordIndex = 0\n",
    "\n",
    "def FillIndex(keyword):\n",
    "    global KeyWordDictionary\n",
    "    global KeyWordIndex\n",
    "    \n",
    "    if keyword not in KeyWordDictionary:\n",
    "            KeyWordDictionary[keyword] = KeyWordIndex\n",
    "            KeyWordIndex = KeyWordIndex + 1\n",
    "            \n",
    "def FillIndices(Vector): \n",
    "    [FillIndex(item) for item in Vector[0]]\n",
    "\n",
    "def InsertZeroes(Vector) :\n",
    "    global KeyWordDictionary\n",
    "    global KeyWordIndex\n",
    "    global KeyWordList\n",
    "    \n",
    "    index = 0\n",
    "    indexDict = {}\n",
    "    newVector = KeyWordIndex*[None]\n",
    "    \n",
    "    for indexVar, row in Vector.iterrows():\n",
    "            indexDict[row[0]] = index\n",
    "            index = index +1\n",
    "    \n",
    "    for key in KeyWordDictionary.keys() :\n",
    "        \n",
    "            if key in indexDict:\n",
    "                newVector[KeyWordDictionary[key]] = Vector.iloc[indexDict[key]].to_list()\n",
    "            else :\n",
    "                empty_list = [key, 0,0,0,0]\n",
    "                newVector[KeyWordDictionary[key]] = empty_list\n",
    "    \n",
    "    return newVector\n",
    "\n",
    "\n",
    "    \n",
    "def ExtractVectorsFromDirectory(directory, FileExtension):\n",
    "    files = glob(path.join(directory,\"*.{}\".format(FileExtension)))\n",
    "    output_matches = [GetKeywords(i) for i in files]\n",
    "    output_matchVectors = [GetKeywordVector(i[0],i[1]) for i in output_matches]\n",
    "    output_matchObfuscateVectors = [GetKeywordVector(i[2],i[1]) for i in output_matches]\n",
    "    \n",
    "    # Now we find all the keywords and fill in the missing spots\n",
    "    resetKeyWordContainers()\n",
    "    [FillIndices(Vector) for Vector in output_matchVectors]\n",
    "    KeyWordList = KeyWordDictionary.keys()\n",
    "    \n",
    "    output_matchVectorsV2 = [InsertZeroes(Vector) for Vector in output_matchVectors]\n",
    "    output_ObfuscatedMatchVectorsV2 = [InsertZeroes(Vector) for Vector in output_matchObfuscateVectors]\n",
    "    \n",
    "    \n",
    "    data_array = np.array(output_matchVectorsV2)\n",
    "    data_arrayValues = data_array[:,:,1]\n",
    "    \n",
    "    dataa_arrayObfuscate = np.array(output_ObfuscatedMatchVectorsV2)\n",
    "    data_arrayObfuscateValues = np.array(dataa_arrayObfuscate[:,:,1])\n",
    "    data_arrayCombined = np.concatenate((data_arrayValues, data_arrayObfuscateValues), axis=1)\n",
    "    \n",
    "    #print(data_arrayObfuscateValues.shape)\n",
    "    #print(data_arrayObfuscateValues)\n",
    "    #print(data_arrayValues.shape)\n",
    "    print(data_arrayCombined.shape)\n",
    "    #print(KeyWordList)\n",
    "    \n",
    "    return data_arrayCombined\n",
    "\n",
    "def method_chi2(data, labels, features = 20):\n",
    "    selecter = SelectKBest(score_func=chi2, k=features)\n",
    "    selecter.fit(data, labels)\n",
    "    string = selecter.get_support()\n",
    "    return selecter.transform(data),string\n",
    "\n",
    "def FeatureReduction():\n",
    "    data= ExtractVectorsFromDirectory(os.getcwd(),'pdf')\n",
    "    labels = np.array([0,0,1,1])\n",
    "    ReducedData, selectedFeatures = method_chi2(data, labels, 400)\n",
    "    return  ReducedData, labels\n",
    "\n",
    "def Classification():\n",
    "    data, labels = FeatureReduction()\n",
    "    \n",
    "    #buffer_test = minmax_scale(data,feature_range=(0, 1),axis = 0)\n",
    "    dataset = np.array(data , \"float32\")\n",
    "    \n",
    "    classifier = MLPClassifier(max_iter=50000, alpha=0.1, activation='tanh', learning_rate='adaptive', random_state=200, tol=0.0000001)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset, labels)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    print(X_train)\n",
    "    print(y_train)\n",
    "    \n",
    "    \n",
    "    print(X_test)\n",
    "    print(y_test)\n",
    "    \n",
    "    predictions = classifier.predict(X_test)\n",
    "    print(predictions)\n",
    "    \n",
    "Classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
